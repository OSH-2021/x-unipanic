[节点信息](https://www.notion.so/5a668d19ae6c42abb98cf5821b6c54dc)

### （以下操作全在root下进行）

## 修改Host、测试连接

将每个节点下的 `/etc/hosts` 添加如下

```bash
10.211.55.17    xq1
10.211.55.18    xq2
10.211.55.19    xq3
```

并且ping测试联通性

```bash
ping xq1
```

## 开启ssh服务，设置免密登陆

修改每台节点的/etc/ssh/sshd_config

```bash
PermitRootLogin yes
PasswordAuthentication yes
```

重启ssh服务

```bash
/etc/init.d/ssh restart
```

设置xq1上的ssh空密钥，并分发到每台主机上

```bash
ssh-keygen

ssh-copy-id -i /root/.ssh/id_rsa.pub xq1
ssh-copy-id -i /root/.ssh/id_rsa.pub xq2
ssh-copy-id -i /root/.ssh/id_rsa.pub xq3

ssh xq1
```

最后一步不用口令直接成功登录即可

## admin主机配置

安装ceph-deploy并且建立工作集群

```bash
apt install ceph-deploy

mkdir ceph-cluster
cd ceph-cluster

ceph-deploy new xq1
```

此时xq1为要作为监控节点

## 在其他主机上安装ceph

在xq1上执行如下命令

```bash
ceph-deploy install xq1 xq2 xq3
```

## 初始化集群监控、收集密钥

在xq1上执行如下命令

```bash
ceph-deploy mon xq1
ceph-deploy gatherkets xq1
```

## 添加从机硬盘设备

以xq2为例子，在xq2上添加一块lvm块设备

```bash
apt install lvm2
#创建、挂载lvm硬盘到/srv/ceph/osd0
dd if=/dev/zero of=ceph-volumes.img bs=1M count=8192 oflag=direct
sgdisk -g --clear ceph-volumes.img
sudo vgcreate ceph-volumes $(sudo losetup --show -f ceph-volumes.img)
sudo lvcreate -L2G -nceph0 ceph-volumes
sudo mkfs.xfs -f /dev/ceph-volumes/ceph0
mkdir -p /srv/ceph/osd0
sudo mount /dev/ceph-volumes/ceph0 /srv/ceph/osd0

chmod 777 /srv/ceph/osd0
chmod 777 /srv/ceph/osd1
```

xq3上同理

## 主机上添加两块osd硬盘

在xq1上运行

```bash
ceph-deploy osd prepare xq2:/srv/ceph/osd0 xq3:/srv/ceph/osd0
ceph-deploy osd activate xq2:/srv/ceph/osd0 xq3:/srv/ceph/osd0
```

## 复制管理密钥到其他节点

在xq1上运行

```bash
ceph-deploy --overwrite-conf admin xq1 xq2 xq3
```

## 验证工作

在xq1上运行

```bash
ceph osd tree
```

显示出几个正确的设备信息就表示部署成功

## 测试速度

在xq1上运行

```bash
ceph osd pool create scbench 64 64
rados bench -p scbench 10 write
```

![ubuntu-multi](https://github.com/OSH-2021/x-unipanic/blob/master/Lab4/img/ubuntu-multi.png)

